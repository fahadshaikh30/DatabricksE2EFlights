{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbf9a37-67b6-4504-b834-42b9263f28e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#import dlt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5255f59e-b637-48bc-b954-ed7409ab2117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/Volumes/workspace/bronze/bronzevolume/flights/data/\")\n",
    "df = df.withColumn(\"modifiedDate\", current_timestamp())\\\n",
    "    .withColumn(\"flight_date\", to_date(col(\"flight_date\")))\\\n",
    "    .drop(\"_rescued_data\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "270dd27f-95f6-42e3-8dfc-5ae5308c13e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/Volumes/workspace/bronze/bronzevolume/passengers/data/\")\n",
    "# df = df.withColumn(\"modifiedDate\", current_timestamp())\\\n",
    "#     .withColumn(\"flight_date\", to_date(col(\"flight_date\")))\\\n",
    "#     .drop(\"_rescued_data\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3017d26d-1659-42bf-a1ef-2a6169175fcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a streaming table using dlt. This will load the table incrementally and perform no transformations since its on the staging environment.\n",
    "@dlt.table(\n",
    "  name=\"stage_bookings\"\n",
    "  )\n",
    "def stage_bookings():\n",
    "  df = spark.readStream.format(\"delta\")\\\n",
    "    .load(\"/Volumes/workspace/bronze/bronzevolume/bookings/data/\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b547c523-c4a3-460b-b3a3-d0be1f63ca11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a streaming view and performing transformations on the data\n",
    "@dlt.view(\n",
    "    name=\"trans_bookings\"\n",
    ")\n",
    "def trans_bookings():\n",
    "  df = spark.readStream.table(\"stage_bookings\")\n",
    "  df = df.withColumn(\"amount\", col(\"amount\").cast(DoubleType()))\\\n",
    "    .withColumn(\"modifiedDate\", current_timestamp())\\\n",
    "    .withColumn(\"booking_date\", to_date(col(\"booking_date\")))\\\n",
    "    .drop(\"_rescued_data\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee9ec4c1-e7e0-4fb4-87ec-35834c8802d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# These are the rules that are defined for the silver table\n",
    "rules = {\n",
    "    \"rule1\" : \"booking_id IS NOT NULL\",\n",
    "    \"rule2\" : \"passenger_id IS NOT NULL\",\n",
    "    # \"rule3\" : \"flight_id IS NOT NULL\",\n",
    "    # \"rule4\" : \"airport_id IS NOT NULL\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cc3bc69-9ca9-4048-95b1-accfce02dd64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a streaming table using dlt. This will load the table to a steaming silver table.\n",
    "@dlt.table(\n",
    "    name=\"silver_bookings\"\n",
    ")\n",
    "@dlt.expect_all(rules)\n",
    "def silver_bookings():\n",
    "  df = spark.readStream.table(\"trans_bookings\")\n",
    "  return df  "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SilverNotebook_dlt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
